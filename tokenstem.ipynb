{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How long the program took to acquire the text characteristics?**\n",
    "\n",
    "-> 0.9574556350708008 s\n",
    "\n",
    "**How the program handles:**\n",
    "\n",
    " **A. Upper and lower case words (e.g. \"People\", \"people\", \"Apple\", \"apple\")**\n",
    " \n",
    " -> All the words are converted to lower case and then tokenized\n",
    " \n",
    " **B. Words with dashes (e.g. \"1996-97\", \"middle-class\", \"30-year\", \"tean-ager\")**\n",
    " \n",
    " -> Hyphen is removed from compound words so middle-class becomes middleclass\n",
    " \n",
    " **C. Possessives (e.g. \"sheriff's\", \"university's\")**\n",
    " \n",
    " -> Apostrophee is also removed from words so sheriff's becomes sherrifs\n",
    " \n",
    " **D. Acronyms (e.g., \"U.S.\", \"U.N.\")**\n",
    " \n",
    " -> Dot(.) from acronyms are removed so U.S. becomes us\n",
    " \n",
    "-> Numbers and digits are also tokenized\n",
    "\n",
    "**Briefly discuss your major algorithms and data structures.**\n",
    "\n",
    "-> To save results of tokenization and stemming dictionaries are used so that the retrieval and insertion of data takes o(1)\n",
    "\n",
    "-> Dictionary stores data as {string(token or stem):integer(frequency)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "        \n",
    "    def __init__(self,path): \n",
    "        self.Cranfield_path=path\n",
    "        self.totalCount=0\n",
    "        self.dictionary={}\n",
    "        self.stopwords=self.createStopwordsdict()#Created stop words dictionary\n",
    "        self.fileCount=0\n",
    "        self.avgTokensPerdoc=0.0\n",
    "    \n",
    "    def createStopwordsdict(self):\n",
    "        stopwords=['ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than']\n",
    "        stopwordsDict={}\n",
    "        for i in stopwords:\n",
    "            stopwordsDict[i]=1\n",
    "        return stopwordsDict\n",
    "    \n",
    "    def tokenize(self):\n",
    "        for filename in os.listdir(self.Cranfield_path):\n",
    "            self.fileCount+=1\n",
    "            with open('\\\\'.join((self.Cranfield_path,filename))) as f:\n",
    "                for currline in f.readlines():\n",
    "                    tokenizedLine=currline.split()\n",
    "                    for words in tokenizedLine:\n",
    "                        if not words.startswith('<'):\n",
    "                            if not words in self.stopwords:\n",
    "                                words=words.strip()\n",
    "                                words=words.lower()#Converted every word to lowercase\n",
    "                                words=words.replace('.','')#Removed dot(.)\n",
    "                                words=words.replace('-','')#Removed Hyphen from compound words and merged them\n",
    "                                words=words.replace(',','')#Removed comma(,)\n",
    "                                words=words.replace('(','')#Removed opening bracket (\n",
    "                                words=words.replace(')','')#Removed closing bracket )\n",
    "                                words=words.replace('/','')#Removed both slashes\n",
    "                                words=words.replace('/','')\n",
    "                                words=words.replace(\"'\",'')#Removed apostrophee\n",
    "                                if words!='' and words!=' ' and len(words)>2:#Removed words which contain less than 2 characters\n",
    "                                    self.addTodict(words)\n",
    "        self.dictionary=dict(sorted(self.dictionary.items(), key=lambda x: x[1],reverse=True))                                       \n",
    "    \n",
    "    def addTodict(self,word):\n",
    "        self.totalCount+=1\n",
    "        if word in self.dictionary:\n",
    "            self.dictionary[word]+=1\n",
    "        else:\n",
    "            self.dictionary[word]=1\n",
    "    \n",
    "    def stats(self):\n",
    "        print('Tokenization Stats')\n",
    "        print('Number of Tokens: '+ str(self.totalCount))\n",
    "        print('Number of Unique words: '+str(len(self.dictionary)))\n",
    "        print('Number of words that occur only once: '+str(len([key for key,val in self.dictionary.items() if val==1])))\n",
    "        print('30 Frequent words:')\n",
    "        self.frequentwords(30)\n",
    "        self.avgTokensPerdoc=self.totalCount/self.fileCount\n",
    "        print('Average tokens per document: '+str(self.avgTokensPerdoc))\n",
    "    \n",
    "    def frequentwords(self,n):\n",
    "        i=0\n",
    "        for word,freq in self.dictionary.items():\n",
    "            if i==n:\n",
    "                break\n",
    "            print(word+': '+str(freq))\n",
    "            i+=1\n",
    "    \n",
    "    def getTokenDict(self):\n",
    "        return self.dictionary\n",
    "    \n",
    "    def getfilecount(self):\n",
    "        return self.fileCount\n",
    "    \n",
    "    def outputdict(self):\n",
    "        np.savetxt('tokens.csv',np.array(list(self.dictionary.items())),delimiter=',',fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stemming:\n",
    "    \n",
    "    def __init__(self,filecount): \n",
    "        self.totalCount=0\n",
    "        self.dictionary={}\n",
    "        self.fileCount=filecount\n",
    "        self.avgTokensPerdoc=0.0\n",
    "    \n",
    "    def isvowel(self,char):\n",
    "        if char=='a' or char=='e' or char=='i' or char=='o' or char=='u':\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    #*v*\n",
    "    def containsVowel(self,word):\n",
    "        for i in word:\n",
    "            if self.isvowel(i):\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    #*d\n",
    "    def endsinDoubleCons(self,word):\n",
    "        if len(word)>=2:\n",
    "            if self.isConsonant(word,-1) and self.isConsonant(word,-2):\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    #*o i.e. checks for cvc\n",
    "    def check_O(self,word):\n",
    "        if len(word)>=3:\n",
    "            if self.isConsonant(word,-3) and self.isvowel(word[-2]) and self.isConsonant(word,-1):\n",
    "                char=word[-1]\n",
    "                if char!='w' and char!='y' and char!='x':\n",
    "                    return True\n",
    "        return False\n",
    "        \n",
    "    def isConsonant(self,word,i):\n",
    "        char=word[i]\n",
    "        if not self.isvowel(char):\n",
    "            if char=='y':\n",
    "                if i==0 or abs(i)==len(word):\n",
    "                    return True\n",
    "                elif not self.isvowel(i-1):\n",
    "                    return False\n",
    "                else:\n",
    "                    return True\n",
    "        return False\n",
    "        \n",
    "    #calculates m for each word    \n",
    "    def base_M(self,word):\n",
    "        base=[]\n",
    "        basestr=''\n",
    "        for i in range(len(word)):\n",
    "            if self.isConsonant(word,i):\n",
    "                if i!=0:\n",
    "                    prev=base[-1]\n",
    "                    if prev!='C':\n",
    "                        base.append('C')\n",
    "                else:\n",
    "                    base.append('C')\n",
    "            else:\n",
    "                if i!=0:\n",
    "                    prev=base[-1]\n",
    "                    if prev!='V':\n",
    "                        base.append('V')\n",
    "                else:\n",
    "                    base.append('V')\n",
    "        return ''.join(base).count('VC')\n",
    "                    \n",
    "       \n",
    "    def replace(self,word,original,changed):\n",
    "        m=word.rfind(original)\n",
    "        return word[:m]+changed\n",
    "    \n",
    "    def step1a(self,word):\n",
    "        if word.endswith('sses'):\n",
    "            word=self.replace(word,'sses','ss')\n",
    "        elif word.endswith('ies'):\n",
    "            word=self.replace(word,'ies','i')\n",
    "        elif word.endswith('ss'):\n",
    "            word=self.replace(word,'ss','ss')\n",
    "        elif word.endswith('s'):\n",
    "            word=self.replace(word,'s','')\n",
    "        return word\n",
    "    \n",
    "    def step1b(self,word):\n",
    "        flag=False\n",
    "        if word.endswith('eed'):\n",
    "            m=word.rfind('eed')\n",
    "            result=word[:m]\n",
    "            if self.base_M(result)>0:\n",
    "                word=result\n",
    "                word+='ee'\n",
    "        elif word.endswith('ed'):\n",
    "            m=word.rfind('ed')\n",
    "            result=word[:m]\n",
    "            if self.containsVowel(result):\n",
    "                word=result\n",
    "                flag=True\n",
    "        elif word.endswith('ing'):\n",
    "            m=word.rfind('ing')\n",
    "            result=word[:m]\n",
    "            if self.containsVowel(result):\n",
    "                word=result\n",
    "                flag=True\n",
    "        if flag:\n",
    "            if word.endswith('at') or word.endswith('bl') or word.endswith('iz'):\n",
    "                word+='e'\n",
    "            elif self.endsinDoubleCons(word) and not word.endswith('l') and not word.endswith('s') and not word.endswith('z'):\n",
    "                word=word[:-1]\n",
    "            elif self.base_M(word)==1 and self.check_O(word):\n",
    "                word+='e'\n",
    "        return word\n",
    "    \n",
    "    def step1c(self,word):\n",
    "        if word.endswith('y'):\n",
    "            result = word[:-1]\n",
    "            if self.containsVowel(word):\n",
    "                word=result\n",
    "                word+='i'\n",
    "        return word\n",
    "    \n",
    "    def step2(self,word):\n",
    "        if self.base_M(word)>0:\n",
    "            if word.endswith('ational'):\n",
    "                word=self.replace(word,'ational','ate')\n",
    "            elif word.endswith('tional'):\n",
    "                word=self.replace(word,'tional','tion')\n",
    "            elif word.endswith('enci'):\n",
    "                word=self.replace(word,'enci','ence')\n",
    "            elif word.endswith('anci'):\n",
    "                word=self.replace(word,'anci','ance')\n",
    "            elif word.endswith('izer'):\n",
    "                word=self.replace(word,'izer','ize')\n",
    "            elif word.endswith('abli'):\n",
    "                word=self.replace(word,'abli','able')\n",
    "            elif word.endswith('alli'):\n",
    "                word=self.replace(word,'alli','al')\n",
    "            elif word.endswith('entli'):\n",
    "                word=self.replace(word,'entli','ent')\n",
    "            elif word.endswith('eli'):\n",
    "                word=self.replace(word,'eli','e')\n",
    "            elif word.endswith('ousli'):\n",
    "                word=self.replace(word,'ousli','ous')\n",
    "            elif word.endswith('ization'):\n",
    "                word=self.replace(word,'ization','ize')\n",
    "            elif word.endswith('ation'):\n",
    "                word=self.replace(word,'ation','ate')\n",
    "            elif word.endswith('ator'):\n",
    "                word=self.replace(word,'ator','ate')\n",
    "            elif word.endswith('alism'):\n",
    "                word=self.replace(word,'alism','al')\n",
    "            elif word.endswith('iveness'):\n",
    "                word=self.replace(word,'iveness','ive')\n",
    "            elif word.endswith('fulness'):\n",
    "                word=self.replace(word,'fulness','ful')\n",
    "            elif word.endswith('ousness'):\n",
    "                word=self.replace(word,'ousness','ous')\n",
    "            elif word.endswith('aliti'):\n",
    "                word=self.replace(word,'aliti','al')\n",
    "            elif word.endswith('iviti'):\n",
    "                word=self.replace(word,'iviti','ive')\n",
    "            elif word.endswith('bliti'):\n",
    "                word=self.replace(word,'bliti','ble')\n",
    "        return word\n",
    "    \n",
    "    def step3(self,word):\n",
    "        if self.base_M(word)>0:\n",
    "            if word.endswith('icate'):\n",
    "                word=self.replace(word,'icate','ic')\n",
    "            elif word.endswith('ative'):\n",
    "                word=self.replace(word,'ative','')\n",
    "            elif word.endswith('alize'):\n",
    "                word=self.replace(word,'alize','al')\n",
    "            elif word.endswith('iciti'):\n",
    "                word=self.replace(word,'iciti','ic')\n",
    "            elif word.endswith('ical'):\n",
    "                word=self.replace(word,'ical','ic')\n",
    "            elif word.endswith('ful'):\n",
    "                word=self.replace(word,'ful','')\n",
    "            elif word.endswith('ness'):\n",
    "                word=self.replace(word,'ness','')\n",
    "        return word\n",
    "    \n",
    "    def step4(self,word):\n",
    "        if self.base_M(word)>1:\n",
    "            if word.endswith('al'):\n",
    "                word=self.replace(word,'al','')\n",
    "            elif word.endswith('ance'):\n",
    "                word=self.replace(word,'ance','')\n",
    "            elif word.endswith('ence'):\n",
    "                word=self.replace(word,'ence','')\n",
    "            elif word.endswith('er'):\n",
    "                word=self.replace(word,'er','')\n",
    "            elif word.endswith('ic'):\n",
    "                word=self.replace(word,'ic','')\n",
    "            elif word.endswith('able'):\n",
    "                word=self.replace(word,'able','')\n",
    "            elif word.endswith('ible'):\n",
    "                word=self.replace(word,'ible','')\n",
    "            elif word.endswith('ant'):\n",
    "                word=self.replace(word,'ant','')\n",
    "            elif word.endswith('ement'):\n",
    "                word=self.replace(word,'ement','')\n",
    "            elif word.endswith('ment'):\n",
    "                word=self.replace(word,'ment','')\n",
    "            elif word.endswith('ent'):\n",
    "                word=self.replace(word,'ent','')\n",
    "            elif word.endswith('ou'):\n",
    "                word=self.replace(word,'ou','')\n",
    "            elif word.endswith('ism'):\n",
    "                word=self.replace(word,'ism','')\n",
    "            elif word.endswith('ate'):\n",
    "                word=self.replace(word,'ate','')\n",
    "            elif word.endswith('iti'):\n",
    "                word=self.replace(word,'iti','')\n",
    "            elif word.endswith('ous'):\n",
    "                word=self.replace(word,'ous','')\n",
    "            elif word.endswith('ive'):\n",
    "                word=self.replace(word,'ive','')\n",
    "            elif word.endswith('ize'):\n",
    "                word=self.replace(word,'ize','')\n",
    "            elif word.endswith('ion'):\n",
    "                m=word.rfind('ion')\n",
    "                result=word[:m]\n",
    "                if self.base_M(result)>1 and (result.endswith('s') or result.endswith('t')):\n",
    "                    word=result\n",
    "        return word\n",
    "    \n",
    "    def step5a(self,word):\n",
    "        if word.endswith('e'):\n",
    "            result=word[:-1]\n",
    "            if (self.base_M(result)>1) or (self.base_M(result)==1 and not self.check_O(result)):\n",
    "                word=result\n",
    "        return word\n",
    "    \n",
    "    def step5b(self,word):\n",
    "        if self.base_M(word)>1 and self.endsinDoubleCons(word) and word.endswith('l'):\n",
    "            word=word[:-1]\n",
    "        return word\n",
    "    \n",
    "    def stem(self,tokendict):\n",
    "        for i,count in tokendict.items():\n",
    "            word=i\n",
    "            word=self.step1a(word)\n",
    "            word=self.step1b(word)\n",
    "            word=self.step1c(word)\n",
    "            word=self.step2(word)\n",
    "            word=self.step3(word)\n",
    "            word=self.step4(word)\n",
    "            word=self.step5a(word)\n",
    "            word=self.step5b(word)\n",
    "            self.addtodict(word,count)\n",
    "    \n",
    "    def addtodict(self,word,count):\n",
    "        self.totalCount+=count\n",
    "        if word in self.dictionary:\n",
    "            self.dictionary[word]+=count\n",
    "        else:\n",
    "            self.dictionary[word]=count\n",
    "            \n",
    "    def stats(self):\n",
    "        print('Stemming Stats')\n",
    "        print('Number of distinct stems: '+ str(len(self.dictionary)))\n",
    "        print('Number of stems that occur only once: '+str(len([key for key,val in self.dictionary.items() if val==1])))\n",
    "        print('30 Frequent words:')\n",
    "        self.frequentwords(30)\n",
    "        self.avgTokensPerdoc=self.totalCount/self.fileCount\n",
    "        print('Average stemmed tokens per document: '+str(self.avgTokensPerdoc))\n",
    "        \n",
    "    def frequentwords(self,n):\n",
    "        self.dictionary=dict(sorted(self.dictionary.items(), key=lambda x: x[1],reverse=True))\n",
    "        i=0\n",
    "        for word,freq in self.dictionary.items():\n",
    "            if i==n:\n",
    "                break\n",
    "            print(word+': '+str(freq))\n",
    "            i+=1\n",
    "            \n",
    "    def getStemmingDict(self):\n",
    "        return self.dictionary\n",
    "    \n",
    "    def outputdict(self):\n",
    "        np.savetxt('stems.csv',np.array(list(self.dictionary.items())),delimiter=',',fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Text's path:C:\\Users\\dkg27\\Desktop\\Cranfield\n"
     ]
    }
   ],
   "source": [
    "path = input('Enter Text\\'s path:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Token=Tokenizer(path)\n",
    "start_tokenize_time=time.time()\n",
    "d=Token.tokenize()\n",
    "end_tokenize_time=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization Stats\n",
      "Number of Tokens: 132710\n",
      "Number of Unique words: 12272\n",
      "Number of words that occur only once: 6221\n",
      "30 Frequent words:\n",
      "flow: 1736\n",
      "pressure: 1132\n",
      "number: 964\n",
      "boundary: 897\n",
      "results: 885\n",
      "mach: 816\n",
      "theory: 775\n",
      "layer: 728\n",
      "method: 683\n",
      "shock: 589\n",
      "surface: 558\n",
      "obtained: 539\n",
      "given: 520\n",
      "effects: 510\n",
      "solution: 496\n",
      "heat: 483\n",
      "velocity: 481\n",
      "temperature: 478\n",
      "equations: 477\n",
      "supersonic: 460\n",
      "made: 449\n",
      "ratio: 442\n",
      "body: 439\n",
      "wing: 430\n",
      "presented: 425\n",
      "found: 422\n",
      "experimental: 421\n",
      "laminar: 413\n",
      "conditions: 411\n",
      "effect: 402\n",
      "Average tokens per document: 94.79285714285714\n"
     ]
    }
   ],
   "source": [
    "Token.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 0.5826845169067383 seconds\n"
     ]
    }
   ],
   "source": [
    "print('Time taken to tokenize: '+str(end_tokenize_time-start_tokenize_time)+' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem=Stemming(Token.getfilecount())\n",
    "start_stemming_time=time.time()\n",
    "stem.stem(Token.getTokenDict())\n",
    "end_stemming_time=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming Stats\n",
      "Number of distinct stems: 10562\n",
      "Number of stems that occur only once: 5499\n",
      "30 Frequent words:\n",
      "flow: 1965\n",
      "number: 1336\n",
      "pressure: 1279\n",
      "result: 1069\n",
      "boundari: 926\n",
      "effect: 917\n",
      "method: 883\n",
      "theori: 868\n",
      "layer: 859\n",
      "solution: 847\n",
      "mach: 817\n",
      "equation: 774\n",
      "bodi: 740\n",
      "wing: 710\n",
      "present: 685\n",
      "surface: 661\n",
      "obtain: 632\n",
      "shock: 614\n",
      "distribution: 598\n",
      "problem: 591\n",
      "ratio: 588\n",
      "temperature: 583\n",
      "velociti: 554\n",
      "case: 545\n",
      "given: 520\n",
      "heat: 518\n",
      "test: 518\n",
      "condition: 515\n",
      "plate: 494\n",
      "us: 479\n",
      "Average stemmed tokens per document: 94.79285714285714\n"
     ]
    }
   ],
   "source": [
    "stem.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for stemming: 0.3847775459289551 seconds\n"
     ]
    }
   ],
   "source": [
    "print('Time taken for stemming: '+str(end_stemming_time-start_stemming_time)+' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for acqiuring text characteristics(i.e. perfroming both tokenization and stemming): 0.9674620628356934 seconds\n"
     ]
    }
   ],
   "source": [
    "print('Time taken for acqiuring text characteristics(i.e. perfroming both tokenization and stemming): '+str(end_stemming_time-start_stemming_time+end_tokenize_time-start_tokenize_time)+' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Token.outputdict()\n",
    "stem.outputdict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
